{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementación del modelo original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import jarque_bera\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\kamac\\OneDrive\\Desktop\\MachineLearningUN\\EDA\\ws_modelos.xlsx\"\n",
    "data = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data.drop(columns=['Fecha', 'VelViento100m_1', 'VelViento100m_2', 'VelViento80m_1', 'VelViento80m_2', 'VelViento60m', 'DirViento80m', 'DirViento60m'])\n",
    "y1 = data['VelViento100m_1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.3, random_state = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bayesian Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(max_depth, learning_rate, n_estimators, gamma, min_child_weight):\n",
    "    params = {\n",
    "        \"max_depth\": int(max_depth),\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_estimators\": int(n_estimators),\n",
    "        \"gamma\": gamma,\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    return -rmse  # Maximizar el inverso de RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    \"max_depth\": (3, 20),\n",
    "    \"learning_rate\": (0.01, 0.3),\n",
    "    \"n_estimators\": (50, 300),\n",
    "    \"gamma\": (0, 5),\n",
    "    \"min_child_weight\": (1, 10),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | learni... | max_depth | min_ch... | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-1.461   \u001b[39m | \u001b[39m1.873    \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m15.44    \u001b[39m | \u001b[39m6.388    \u001b[39m | \u001b[39m89.0     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-1.408   \u001b[39m | \u001b[35m0.78     \u001b[39m | \u001b[35m0.02684  \u001b[39m | \u001b[35m17.72    \u001b[39m | \u001b[35m6.41     \u001b[39m | \u001b[35m227.0    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-1.448   \u001b[39m | \u001b[39m0.1029   \u001b[39m | \u001b[39m0.2913   \u001b[39m | \u001b[39m17.15    \u001b[39m | \u001b[39m2.911    \u001b[39m | \u001b[39m95.46    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-1.48    \u001b[39m | \u001b[39m0.917    \u001b[39m | \u001b[39m0.09823  \u001b[39m | \u001b[39m11.92    \u001b[39m | \u001b[39m4.888    \u001b[39m | \u001b[39m122.8    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-1.722   \u001b[39m | \u001b[39m3.059    \u001b[39m | \u001b[39m0.05045  \u001b[39m | \u001b[39m7.966    \u001b[39m | \u001b[39m4.297    \u001b[39m | \u001b[39m164.0    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-1.564   \u001b[39m | \u001b[39m3.926    \u001b[39m | \u001b[39m0.06791  \u001b[39m | \u001b[39m11.74    \u001b[39m | \u001b[39m6.332    \u001b[39m | \u001b[39m61.61    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-1.931   \u001b[39m | \u001b[39m3.038    \u001b[39m | \u001b[39m0.05945  \u001b[39m | \u001b[39m4.106    \u001b[39m | \u001b[39m9.54     \u001b[39m | \u001b[39m291.4    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-1.938   \u001b[39m | \u001b[39m4.042    \u001b[39m | \u001b[39m0.09834  \u001b[39m | \u001b[39m4.66     \u001b[39m | \u001b[39m7.158    \u001b[39m | \u001b[39m160.0    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-2.033   \u001b[39m | \u001b[39m0.6102   \u001b[39m | \u001b[39m0.1536   \u001b[39m | \u001b[39m3.585    \u001b[39m | \u001b[39m9.184    \u001b[39m | \u001b[39m114.7    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-1.528   \u001b[39m | \u001b[39m3.313    \u001b[39m | \u001b[39m0.1004   \u001b[39m | \u001b[39m11.84    \u001b[39m | \u001b[39m5.92     \u001b[39m | \u001b[39m96.21    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-1.464   \u001b[39m | \u001b[39m2.919    \u001b[39m | \u001b[39m0.1067   \u001b[39m | \u001b[39m18.47    \u001b[39m | \u001b[39m6.505    \u001b[39m | \u001b[39m219.1    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-1.669   \u001b[39m | \u001b[39m2.547    \u001b[39m | \u001b[39m0.1765   \u001b[39m | \u001b[39m8.887    \u001b[39m | \u001b[39m9.097    \u001b[39m | \u001b[39m224.3    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-1.514   \u001b[39m | \u001b[39m2.655    \u001b[39m | \u001b[39m0.2449   \u001b[39m | \u001b[39m19.82    \u001b[39m | \u001b[39m1.558    \u001b[39m | \u001b[39m232.6    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-1.477   \u001b[39m | \u001b[39m0.9155   \u001b[39m | \u001b[39m0.1944   \u001b[39m | \u001b[39m17.78    \u001b[39m | \u001b[39m1.113    \u001b[39m | \u001b[39m129.4    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-1.491   \u001b[39m | \u001b[39m3.629    \u001b[39m | \u001b[39m0.1575   \u001b[39m | \u001b[39m19.35    \u001b[39m | \u001b[39m9.932    \u001b[39m | \u001b[39m127.2    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-1.411   \u001b[39m | \u001b[39m0.1485   \u001b[39m | \u001b[39m0.09136  \u001b[39m | \u001b[39m19.85    \u001b[39m | \u001b[39m2.087    \u001b[39m | \u001b[39m79.02    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-1.637   \u001b[39m | \u001b[39m2.012    \u001b[39m | \u001b[39m0.1838   \u001b[39m | \u001b[39m8.203    \u001b[39m | \u001b[39m1.38     \u001b[39m | \u001b[39m78.52    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-1.541   \u001b[39m | \u001b[39m4.67     \u001b[39m | \u001b[39m0.2437   \u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m9.351    \u001b[39m | \u001b[39m74.47    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-1.519   \u001b[39m | \u001b[39m3.663    \u001b[39m | \u001b[39m0.2126   \u001b[39m | \u001b[39m18.78    \u001b[39m | \u001b[39m1.179    \u001b[39m | \u001b[39m117.4    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-1.494   \u001b[39m | \u001b[39m2.466    \u001b[39m | \u001b[39m0.1867   \u001b[39m | \u001b[39m19.76    \u001b[39m | \u001b[39m2.292    \u001b[39m | \u001b[39m203.8    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-1.445   \u001b[39m | \u001b[39m0.04893  \u001b[39m | \u001b[39m0.2852   \u001b[39m | \u001b[39m15.84    \u001b[39m | \u001b[39m1.653    \u001b[39m | \u001b[39m189.2    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-1.601   \u001b[39m | \u001b[39m2.798    \u001b[39m | \u001b[39m0.0834   \u001b[39m | \u001b[39m9.775    \u001b[39m | \u001b[39m9.839    \u001b[39m | \u001b[39m195.2    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-1.476   \u001b[39m | \u001b[39m1.824    \u001b[39m | \u001b[39m0.1616   \u001b[39m | \u001b[39m19.62    \u001b[39m | \u001b[39m3.616    \u001b[39m | \u001b[39m180.0    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-1.985   \u001b[39m | \u001b[39m1.032    \u001b[39m | \u001b[39m0.0275   \u001b[39m | \u001b[39m5.541    \u001b[39m | \u001b[39m1.196    \u001b[39m | \u001b[39m181.6    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-1.521   \u001b[39m | \u001b[39m4.957    \u001b[39m | \u001b[39m0.1575   \u001b[39m | \u001b[39m19.32    \u001b[39m | \u001b[39m9.73     \u001b[39m | \u001b[39m187.4    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-1.512   \u001b[39m | \u001b[39m0.08554  \u001b[39m | \u001b[39m0.07888  \u001b[39m | \u001b[39m9.748    \u001b[39m | \u001b[39m8.07     \u001b[39m | \u001b[39m134.6    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-1.53    \u001b[39m | \u001b[39m3.06     \u001b[39m | \u001b[39m0.1984   \u001b[39m | \u001b[39m19.82    \u001b[39m | \u001b[39m1.056    \u001b[39m | \u001b[39m194.2    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-1.519   \u001b[39m | \u001b[39m4.31     \u001b[39m | \u001b[39m0.1058   \u001b[39m | \u001b[39m19.19    \u001b[39m | \u001b[39m1.283    \u001b[39m | \u001b[39m86.77    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-1.442   \u001b[39m | \u001b[39m1.748    \u001b[39m | \u001b[39m0.06296  \u001b[39m | \u001b[39m17.99    \u001b[39m | \u001b[39m9.82     \u001b[39m | \u001b[39m243.1    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-1.412   \u001b[39m | \u001b[39m0.07399  \u001b[39m | \u001b[39m0.2608   \u001b[39m | \u001b[39m19.18    \u001b[39m | \u001b[39m8.787    \u001b[39m | \u001b[39m254.7    \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-1.481   \u001b[39m | \u001b[39m0.2238   \u001b[39m | \u001b[39m0.2302   \u001b[39m | \u001b[39m10.66    \u001b[39m | \u001b[39m3.76     \u001b[39m | \u001b[39m250.3    \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-1.525   \u001b[39m | \u001b[39m4.376    \u001b[39m | \u001b[39m0.1831   \u001b[39m | \u001b[39m19.88    \u001b[39m | \u001b[39m2.627    \u001b[39m | \u001b[39m262.1    \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-1.59    \u001b[39m | \u001b[39m1.927    \u001b[39m | \u001b[39m0.1024   \u001b[39m | \u001b[39m9.118    \u001b[39m | \u001b[39m9.966    \u001b[39m | \u001b[39m260.4    \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-1.461   \u001b[39m | \u001b[39m1.138    \u001b[39m | \u001b[39m0.02426  \u001b[39m | \u001b[39m19.72    \u001b[39m | \u001b[39m1.49     \u001b[39m | \u001b[39m247.7    \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-1.96    \u001b[39m | \u001b[39m0.2153   \u001b[39m | \u001b[39m0.1177   \u001b[39m | \u001b[39m3.417    \u001b[39m | \u001b[39m9.256    \u001b[39m | \u001b[39m241.4    \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m-1.45    \u001b[39m | \u001b[39m0.9581   \u001b[39m | \u001b[39m0.09513  \u001b[39m | \u001b[39m19.35    \u001b[39m | \u001b[39m2.524    \u001b[39m | \u001b[39m50.0     \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m-2.045   \u001b[39m | \u001b[39m1.244    \u001b[39m | \u001b[39m0.05462  \u001b[39m | \u001b[39m6.504    \u001b[39m | \u001b[39m1.706    \u001b[39m | \u001b[39m50.59    \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m-1.493   \u001b[39m | \u001b[39m1.768    \u001b[39m | \u001b[39m0.1658   \u001b[39m | \u001b[39m19.94    \u001b[39m | \u001b[39m1.562    \u001b[39m | \u001b[39m66.24    \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m-1.527   \u001b[39m | \u001b[39m3.107    \u001b[39m | \u001b[39m0.2732   \u001b[39m | \u001b[39m19.56    \u001b[39m | \u001b[39m9.996    \u001b[39m | \u001b[39m56.2     \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m-1.586   \u001b[39m | \u001b[39m0.1743   \u001b[39m | \u001b[39m0.08653  \u001b[39m | \u001b[39m7.682    \u001b[39m | \u001b[39m1.155    \u001b[39m | \u001b[39m209.7    \u001b[39m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_xgboost,\n",
    "    pbounds=param_bounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimizer.max['params']\n",
    "best_model = xgb.XGBRegressor(\n",
    "    max_depth=int(best_params['max_depth']),\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=int(best_params['n_estimators']),\n",
    "    gamma=best_params['gamma'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "predictions = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4076855242944042\n",
      "R2 Score: 0.8960165627720081\n",
      "Ljung-Box p-value: 0.45843638807833753\n",
      "Jarque-Bera p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "r2 = r2_score(y_test, predictions)\n",
    "residuals = y_test - predictions\n",
    "ljung_box_p_value = acorr_ljungbox(residuals, lags=[30], return_df=True)['lb_pvalue'].iloc[0]\n",
    "jarque_bera_p_value = jarque_bera(residuals)[1]\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"Ljung-Box p-value: {ljung_box_p_value}\")\n",
    "print(f\"Jarque-Bera p-value: {jarque_bera_p_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import jarque_bera\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def evaluate_xgboost_with_gridsearch_r2(X_train, y_train, X_test, y_test, results, resultados_graficos):\n",
    "    # Definimos el espacio de búsqueda para GridSearchCV\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3]\n",
    "    }\n",
    "\n",
    "    # Inicializamos el modelo XGBoost\n",
    "    modelo = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    # Implementamos GridSearchCV con 10 pliegues\n",
    "    grid_xgb = GridSearchCV(\n",
    "        estimator=modelo,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring='r2',  # Usamos R² como métrica de evaluación\n",
    "        cv=10,         # Validación cruzada con 10 pliegues\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajustamos el modelo a los datos de entrenamiento\n",
    "    grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predicciones para entrenamiento y prueba\n",
    "    y_train_pred = grid_xgb.predict(X_train)\n",
    "    y_pred_xgb = grid_xgb.predict(X_test)\n",
    "\n",
    "    # Residuos\n",
    "    residuals = y_test - y_pred_xgb\n",
    "\n",
    "    # Métricas de evaluación\n",
    "    r2 = r2_score(y_test, y_pred_xgb)\n",
    "    mape = np.mean(np.abs((y_test - y_pred_xgb) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "\n",
    "    # Pruebas estadísticas para residuos\n",
    "    ljung_box_p_value = acorr_ljungbox(residuals, lags=[30], return_df=True)['lb_pvalue'].iloc[0]\n",
    "    jarque_bera_p_value = jarque_bera(residuals)[1]\n",
    "\n",
    "    # Guardamos los resultados en las listas proporcionadas\n",
    "    results.append({\n",
    "        'Modelo': 'XGBoost Regressor (GridSearch)',\n",
    "        'MAPE': mape,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Ljung-Box p-value': ljung_box_p_value,\n",
    "        'Jarque-Bera p-value': jarque_bera_p_value\n",
    "    })\n",
    "\n",
    "    resultados_graficos.append({\n",
    "        'nombre_modelo': 'XGBoost Regressor (GridSearch)',\n",
    "        'y_train': y_train,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test': y_test,\n",
    "        'y_test_pred': y_pred_xgb,\n",
    "        'residuos': residuals\n",
    "    })\n",
    "\n",
    "    print(\"Mejores hiperparámetros encontrados:\", grid_xgb.best_params_)\n",
    "    print(\"Mejor puntuación R² en validación cruzada:\", grid_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\sklearn.py\", line 1081, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\sklearn.py\", line 596, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\sklearn.py\", line 1003, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 1573, in __init__\n",
      "    self._init(\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 1632, in _init\n",
      "    it.reraise()\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 569, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 550, in _handle_exception\n",
      "    return fn()\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 637, in <lambda>\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\data.py\", line 1388, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 626, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 954, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 1092, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\data.py\", line 1334, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\data.py\", line 679, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\data.py\", line 1267, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [10:26:51] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\kamac\\miniconda3\\envs\\data_viz\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.42882467 0.61256275 0.14176893 0.50474728 0.70475496\n",
      " 0.14864936 0.5261543  0.73185515 0.14954292 0.52843659 0.73432452\n",
      " 0.43407118 0.76264409 0.80218108 0.51074454 0.84616968 0.87404741\n",
      " 0.532387   0.87038657 0.88992872 0.53492749 0.87146163 0.88829819\n",
      " 0.62178639 0.80242917 0.82421961 0.71490044 0.8741934  0.88409942\n",
      " 0.74267598 0.89079509 0.89416667 0.74548318 0.88708294 0.88912589\n",
      " 0.77507809 0.83388272 0.84966055 0.85544672 0.88116398 0.88399216\n",
      " 0.87528853 0.88670103 0.88697869 0.87686176 0.88487487 0.88489215]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados: {'learning_rate': 0.1, 'max_depth': 15, 'n_estimators': 100}\n",
      "Mejor puntuación R² en validación cruzada: 0.8941666710088407\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "resultados_graficos = []\n",
    "\n",
    "evaluate_xgboost_with_gridsearch_r2(X_train, y_train, X_test, y_test, results, resultados_graficos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "      <th>Ljung-Box p-value</th>\n",
       "      <th>Jarque-Bera p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Regressor (GridSearch)</td>\n",
       "      <td>11.475852</td>\n",
       "      <td>0.972034</td>\n",
       "      <td>2.008381</td>\n",
       "      <td>1.417173</td>\n",
       "      <td>0.89461</td>\n",
       "      <td>0.402368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo       MAPE       MAE       MSE      RMSE  \\\n",
       "0  XGBoost Regressor (GridSearch)  11.475852  0.972034  2.008381  1.417173   \n",
       "\n",
       "        R²  Ljung-Box p-value  Jarque-Bera p-value  \n",
       "0  0.89461           0.402368                  0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados = pd.DataFrame(results)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GWO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1/20: Mejor RMSE = 1.4266196351997413\n",
      "Iteración 2/20: Mejor RMSE = 1.403670040199491\n",
      "Iteración 3/20: Mejor RMSE = 1.402758042439684\n",
      "Iteración 4/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 5/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 6/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 7/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 8/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 9/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 10/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 11/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 12/20: Mejor RMSE = 1.4000905332402367\n",
      "Iteración 13/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 14/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 15/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 16/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 17/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 18/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 19/20: Mejor RMSE = 1.3947734584687395\n",
      "Iteración 20/20: Mejor RMSE = 1.3947734584687395\n",
      "Mejores hiperparámetros encontrados con GWO: {'max_depth': 5, 'learning_rate': 0.08080947168577321, 'n_estimators': 133, 'best_rmse': 1.3947734584687395}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random\n",
    "\n",
    "# Función objetivo para optimización\n",
    "def objective_function(params, X_train, y_train, X_test, y_test):\n",
    "    max_depth, learning_rate, n_estimators = params\n",
    "    model = XGBRegressor(\n",
    "        max_depth=int(max_depth),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    return rmse\n",
    "\n",
    "# Implementación de GWO\n",
    "def gwo_xgboost(X_train, y_train, X_test, y_test, n_wolves=5, n_iterations=20):\n",
    "    # Definir espacio de búsqueda\n",
    "    bounds = {\n",
    "        'max_depth': (3, 30),        # Profundidad del árbol\n",
    "        'learning_rate': (0.01, 0.3), # Tasa de aprendizaje\n",
    "        'n_estimators': (50, 300)    # Número de árboles\n",
    "    }\n",
    "\n",
    "    # Inicialización de lobos\n",
    "    wolves = [np.array([random.uniform(bounds[k][0], bounds[k][1]) for k in bounds.keys()]) for _ in range(n_wolves)]\n",
    "    alpha, beta, delta = None, None, None  # Tres mejores lobos\n",
    "    alpha_score, beta_score, delta_score = float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        for wolf in wolves:\n",
    "            score = objective_function(wolf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            # Actualizar lobos alfa, beta y delta\n",
    "            if score < alpha_score:\n",
    "                delta, beta, alpha = beta, alpha, wolf\n",
    "                delta_score, beta_score, alpha_score = beta_score, alpha_score, score\n",
    "            elif score < beta_score:\n",
    "                delta, beta = beta, wolf\n",
    "                delta_score, beta_score = beta_score, score\n",
    "            elif score < delta_score:\n",
    "                delta = wolf\n",
    "                delta_score = score\n",
    "\n",
    "        # Actualizar posiciones de los lobos\n",
    "        for i, wolf in enumerate(wolves):\n",
    "            for j in range(len(wolf)):\n",
    "                A1 = 2 * random.random() - 1\n",
    "                C1 = 2 * random.random()\n",
    "                X1 = alpha[j] - A1 * abs(C1 * alpha[j] - wolf[j])\n",
    "\n",
    "                A2 = 2 * random.random() - 1\n",
    "                C2 = 2 * random.random()\n",
    "                X2 = beta[j] - A2 * abs(C2 * beta[j] - wolf[j])\n",
    "\n",
    "                A3 = 2 * random.random() - 1\n",
    "                C3 = 2 * random.random()\n",
    "                X3 = delta[j] - A3 * abs(C3 * delta[j] - wolf[j])\n",
    "\n",
    "                wolf[j] = (X1 + X2 + X3) / 3  # Actualización promedio\n",
    "\n",
    "                # Restringir al espacio de búsqueda\n",
    "                wolf[j] = np.clip(wolf[j], bounds[list(bounds.keys())[j]][0], bounds[list(bounds.keys())[j]][1])\n",
    "\n",
    "        print(f\"Iteración {iteration+1}/{n_iterations}: Mejor RMSE = {alpha_score}\")\n",
    "\n",
    "    # Devuelve los mejores parámetros\n",
    "    return {\n",
    "        'max_depth': int(alpha[0]),\n",
    "        'learning_rate': alpha[1],\n",
    "        'n_estimators': int(alpha[2]),\n",
    "        'best_rmse': alpha_score\n",
    "}\n",
    "\n",
    "# Llamar a la optimización GWO\n",
    "best_params_gwo = gwo_xgboost(X_train, y_train, X_test, y_test)\n",
    "print(\"Mejores hiperparámetros encontrados con GWO:\", best_params_gwo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1/20: Mejor RMSE Test = 1.4072\n",
      "Iteración 2/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 3/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 4/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 5/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 6/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 7/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 8/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 9/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 10/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 11/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 12/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 13/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 14/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 15/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 16/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 17/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 18/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 19/20: Mejor RMSE Test = 1.4041\n",
      "Iteración 20/20: Mejor RMSE Test = 1.4041\n",
      "\n",
      "Resultados finales de WOA-XGBoost:\n",
      "R² Train: 0.9999\n",
      "R² Test: 0.8965\n",
      "RMSE Train: 0.0407\n",
      "RMSE Test: 1.4041\n",
      "Hiperparámetros óptimos: {'max_depth': 3, 'learning_rate': 0.016485796467220086, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random\n",
    "\n",
    "# Función para evaluar las métricas\n",
    "def evaluate_woa_metrics(params, X_train, y_train, X_test, y_test):\n",
    "    max_depth, learning_rate, n_estimators = params\n",
    "    model = XGBRegressor(\n",
    "        max_depth=int(max_depth),\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=int(n_estimators),\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Métricas\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    return rmse_test, r2_train, r2_test, rmse_train, rmse_test, model\n",
    "\n",
    "# Implementación del Whale Optimization Algorithm (WOA)\n",
    "def woa_xgboost(X_train, y_train, X_test, y_test, n_whales=10, n_iterations=20):\n",
    "    # Definir espacio de búsqueda\n",
    "    bounds = {\n",
    "        'max_depth': (3, 20),         # Profundidad del árbol\n",
    "        'learning_rate': (0.01, 0.3), # Tasa de aprendizaje\n",
    "        'n_estimators': (50, 300)     # Número de árboles\n",
    "    }\n",
    "\n",
    "    # Inicialización de ballenas (aleatorio dentro del espacio de búsqueda)\n",
    "    whales = [np.array([random.uniform(bounds[k][0], bounds[k][1]) for k in bounds.keys()]) for _ in range(n_whales)]\n",
    "    best_whale, best_score = None, float(\"inf\")\n",
    "    best_r2_train, best_r2_test, best_rmse_train, best_rmse_test, best_model = None, None, None, None, None\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        for i, whale in enumerate(whales):\n",
    "            # Evaluar métricas\n",
    "            rmse_test, r2_train, r2_test, rmse_train, rmse_test_value, model = evaluate_woa_metrics(whale, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            # Actualizar la mejor ballena\n",
    "            if rmse_test_value < best_score:\n",
    "                best_whale = whale\n",
    "                best_score = rmse_test_value\n",
    "                best_r2_train, best_r2_test, best_rmse_train, best_rmse_test, best_model = r2_train, r2_test, rmse_train, rmse_test_value, model\n",
    "\n",
    "        # Actualizar posiciones de las ballenas (comportamiento de caza)\n",
    "        for i, whale in enumerate(whales):\n",
    "            for j in range(len(whale)):\n",
    "                r = random.random()\n",
    "                if r < 0.5:\n",
    "                    # Movimiento en espiral\n",
    "                    D = abs(best_whale[j] - whale[j])\n",
    "                    b = 1  # Constante de forma para el espiral\n",
    "                    l = random.uniform(-1, 1)  # Factor aleatorio\n",
    "                    whale[j] = D * np.exp(b * l) * np.cos(2 * np.pi * l) + best_whale[j]\n",
    "                else:\n",
    "                    # Movimiento lineal hacia la mejor ballena\n",
    "                    A = 2 * random.random() - 1\n",
    "                    C = 2 * random.random()\n",
    "                    D = abs(C * best_whale[j] - whale[j])\n",
    "                    whale[j] = best_whale[j] - A * D\n",
    "\n",
    "                # Restringir al espacio de búsqueda\n",
    "                whale[j] = np.clip(whale[j], bounds[list(bounds.keys())[j]][0], bounds[list(bounds.keys())[j]][1])\n",
    "\n",
    "        print(f\"Iteración {iteration+1}/{n_iterations}: Mejor RMSE Test = {best_score:.4f}\")\n",
    "\n",
    "    # Devuelve los mejores parámetros y métricas\n",
    "    return {\n",
    "        'best_params': {\n",
    "            'max_depth': int(best_whale[0]),\n",
    "            'learning_rate': best_whale[1],\n",
    "            'n_estimators': int(best_whale[2])\n",
    "        },\n",
    "        'best_r2_train': best_r2_train,\n",
    "        'best_r2_test': best_r2_test,\n",
    "        'best_rmse_train': best_rmse_train,\n",
    "        'best_rmse_test': best_rmse_test,\n",
    "        'best_model': best_model\n",
    "    }\n",
    "\n",
    "# Llamar a la optimización WOA\n",
    "results_woa = woa_xgboost(X_train, y_train, X_test, y_test)\n",
    "print(\"\\nResultados finales de WOA-XGBoost:\")\n",
    "print(f\"R² Train: {results_woa['best_r2_train']:.4f}\")\n",
    "print(f\"R² Test: {results_woa['best_r2_test']:.4f}\")\n",
    "print(f\"RMSE Train: {results_woa['best_rmse_train']:.4f}\")\n",
    "print(f\"RMSE Test: {results_woa['best_rmse_test']:.4f}\")\n",
    "print(f\"Hiperparámetros óptimos: {results_woa['best_params']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
